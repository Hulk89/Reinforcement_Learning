{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN)\n",
    "\n",
    "드디어 DQN이다! DQN은 Q-Network을 improve한 버전으로 다음 세가지 큰 차이가 있다.\n",
    "\n",
    "1. layer가 하나인 NN -> 여러층의 CNN\n",
    "2. Experience Replay\n",
    "  * agent의 경험을 저장해두고 이를 랜덤하게 뽑아서 batch를 만들어 학습에 이용\n",
    "  * `<state, action, reward, next state>`로 이루어짐\n",
    "  * 고정된 크기의 버퍼가 존재하며, 새로 들어온 녀석이 오래전 녀석을 밀어낸다.\n",
    "3. target Q-value($max Q(s, a)$)를 계산하는 target NN을 따로 사용\n",
    "  * 이렇게하면 더 안정적으로 학습이 가능하다고한다. [딥마인드 논문](https://arxiv.org/pdf/1509.02971.pdf)\n",
    "\n",
    "## And More...\n",
    "여기에 DQN의 성능을 올리기 위해 사용된 기법 두가지를 더해보자.\n",
    "\n",
    "### Double DQN\n",
    "* 관찰: 자주 보이는 state에 대한 action의 Q-value를 과대평가한다.\n",
    "* 수정: target Q-value를 구할 때, max값을 취하는 것이 아니라 주요 신경망이 action을 선택하고, 그 행동에 대한 target Q-value를 가져온다.\n",
    "$\\hat{Q}(s,a) = r(s,a) + \\gamma \\cdot Q(s', argmax_{a_i}Q(s', a_i, \\theta 1), \\theta 2)$\n",
    "> $\\theta 1$은 주요 network의 파라미터, $\\theta 2$는 target Q-value를 계산하는 network의 파라미터이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dueling DQN\n",
    "\n",
    "* Q(s, a): 어떤 상태에서 어떤 행동을 취했을 때의 reward\n",
    "* V(s): 상태에 대한 reward\n",
    "* A(a): 행동에 대한 reward\n",
    "* Q(s, a) = V(s) + A(a)\n",
    "\n",
    "위처럼 Q(s, a)를 두가지로 분리해서 V(s), A(a)를 계산하고 이를 합쳐서 Q(s, a)를 구하는 방식이다.\n",
    "\n",
    "# Coding~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 게임환경\n",
    "agent는 파란색 사각형이며 빨간색 사각형을 피하고(R:-1) 초록색 사각형(R: 1)으로 이동하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114594390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import gameEnv\n",
    "\n",
    "env = gameEnv(partial=False, size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 신경망 구현\n",
    "class Qnetwork():\n",
    "    def __init__(self):\n",
    "        # 21168 = 84*84*3\n",
    "        self.scalarInput = tf.placeholder(shape=[None, 21168],\n",
    "                                          dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,\n",
    "                                  shape=[-1, 84, 84, 3])\n",
    "        \n",
    "        # (image_size - filter_size) / stride + 1 \n",
    "        # output: [B x 20 x 20 x 32]\n",
    "        self.conv1 = tf.contrib.layers.conv2d(\n",
    "            inputs=self.imageIn,\n",
    "            num_outputs=32,\n",
    "            kernel_size=[8,8],\n",
    "            stride=[4,4],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None)\n",
    "        # output: [B x 9 x 9 x 64]\n",
    "        self.conv2 = tf.contrib.layers.conv2d(\n",
    "            inputs=self.conv1,\n",
    "            num_outputs=64,\n",
    "            kernel_size=[4,4],\n",
    "            stride=[2,2],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None)\n",
    "\n",
    "        # output: [B x 7 x 7 x 64]\n",
    "        self.conv3 = tf.contrib.layers.convolution2d(\n",
    "            inputs=self.conv2,\n",
    "            num_outputs=64,\n",
    "            kernel_size=[3,3],\n",
    "            stride=[1,1],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None)\n",
    "\n",
    "        # output: [B x 1 x 1 x 512]\n",
    "        self.conv4 = tf.contrib.layers.convolution2d(\n",
    "            inputs=self.conv3,\n",
    "            num_outputs=512,\n",
    "            kernel_size=[7,7],\n",
    "            stride=[1,1],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None)\n",
    "        \n",
    "        ############### Dueling DQN 적용 ################\n",
    "        # 마지막 콘볼루션 레이어의 출력을 2로 나눈다.\n",
    "        # streamAC, streamVC 는 각각 [B x 1 x 1 x 256]\n",
    "        self.streamAC, self.streamVC = tf.split(self.conv4, 2, 3)\n",
    "        \n",
    "        # A flattened tensor with shape [batch_size, k].\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        \n",
    "        # 256개의 노드를 곱해서 각각 A와 V를 구하는 가중치\n",
    "        self.AW = tf.Variable(tf.random_normal([256, env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([256, 1]))\n",
    "        # 점수화 한다.\n",
    "        self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "        self.Value = tf.matmul(self.streamV, self.VW)\n",
    "        # activation function은 없네...\n",
    "        \n",
    "        # normalize except std.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,\n",
    "                                             tf.reduce_mean(self.Advantage,\n",
    "                                                            reduction_indices=1,\n",
    "                                                            keep_dims=True))\n",
    "        ###############################################\n",
    "        # get action\n",
    "        self.predict = tf.argmax(self.Qout, 1)\n",
    "        \n",
    "        # 타겟과 예측 Q value 사이의 차이의 제곱합이 손실이다.\n",
    "        # 타겟Q를 받는 부분\n",
    "        self.targetQ = tf.placeholder(shape=[None],\n",
    "                                      dtype=tf.float32)\n",
    "        # 행동을 받는 부분\n",
    "        self.actions = tf.placeholder(shape=[None],\n",
    "                                      dtype=tf.int32)\n",
    "\n",
    "        self.actions_onehot = tf.one_hot(self.actions,\n",
    "                                        env.actions,\n",
    "                                        on_value=1.0,\n",
    "                                        off_value=0.0)\n",
    "\n",
    "        # 각 네트워크의 행동의 Q 값을 골라내는 것\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        # 각각의 차이\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        # 손실\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        # 최적화 방법 adam\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        # 업데이트 함수\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size=50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience) + len(self.buffer))\n",
    "                         - self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "\n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)), [size, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flattenState(states):\n",
    "    '''\n",
    "    단순히 화면을 flatten시켜주는 함수\n",
    "    '''\n",
    "    return np.reshape(states,[21168])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars, tau):\n",
    "    # tfVars: 학습 가능한 변수들\n",
    "    # tau: 타겟 신경망이 학습 신경망을 향하는 비율\n",
    "\n",
    "    # 학습 가능한 변수들의 수\n",
    "    total_vars = len(tfVars)\n",
    "    \n",
    "    # 연산자 저장 리스트\n",
    "    op_holder = []\n",
    "    # 학습 가능한 변수의 절반은 주요 신경망, 절반은 타겟 신경망\n",
    "    for idx, var in enumerate(tfVars[0:int(total_vars/2)]):\n",
    "        # 앞의 절반의 값에 tau 값을 곱하면, 주요 신경망의 weight에 곱해지고\n",
    "        # 뒤의 절반의 값에 1-tau 값을 곱하면, 타겟 신경망의 weight에 곱해져서\n",
    "        idx_plus_half = int(idx)+int(total_vars/2)\n",
    "        # 이부분 타겟 신경망을 업데이트하는 부분\n",
    "        op_holder.append(tfVars[idx_plus_half].assign(tau * var.value()\n",
    "                                                      + (1 - tau) * tfVars[idx_plus_half].value()))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder, sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "update_freq = 4 # 학습 단계를 얼마나 자주 수행할 것인가\n",
    "y = .99 \n",
    "startE = 1\n",
    "endE = 0.1\n",
    "anneling_steps = 10000. # startE부터 endE까지 몇단계에 걸쳐서 줄일 것인가.\n",
    "num_episodes = 10000\n",
    "pre_train_steps = 10000 # 학습 시작 전에 몇번의 무작위 행위를 할 것인가.\n",
    "max_epLength = 50\n",
    "load_model = False\n",
    "path = \"./dqn\"\n",
    "h_size = 512\n",
    "tau = 0.001 # 주요 신경망을 향해 타겟 신경망이 업데이트되는 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "500 2.3 1\n",
      "1000 1.1 1\n",
      "1500 1.4 1\n",
      "2000 1.1 1\n",
      "2500 0.9 1\n",
      "3000 1.7 1\n",
      "3500 2.2 1\n",
      "4000 1.3 1\n",
      "4500 1.5 1\n",
      "5000 1.3 1\n",
      "5500 3.5 1\n",
      "6000 1.7 1\n",
      "6500 1.8 1\n",
      "7000 2.0 1\n",
      "7500 2.6 1\n",
      "8000 0.4 1\n",
      "8500 1.9 1\n",
      "9000 1.7 1\n",
      "9500 2.9 1\n",
      "10000 3.6 1\n",
      "10500 3.1 0.9549999999999828\n",
      "11000 1.6 0.9099999999999655\n",
      "11500 1.2 0.8649999999999483\n",
      "12000 1.3 0.819999999999931\n",
      "12500 2.2 0.7749999999999138\n",
      "13000 1.8 0.7299999999998965\n",
      "13500 1.2 0.6849999999998793\n",
      "14000 3.1 0.639999999999862\n",
      "14500 2.1 0.5949999999998448\n",
      "15000 2.1 0.5499999999998275\n",
      "15500 1.2 0.5049999999998103\n",
      "16000 1.6 0.4599999999998177\n",
      "16500 0.4 0.41499999999982823\n",
      "17000 1.4 0.36999999999983874\n",
      "17500 1.7 0.32499999999984924\n",
      "18000 0.2 0.27999999999985975\n",
      "18500 1.0 0.23499999999986562\n",
      "19000 0.0 0.18999999999986225\n",
      "19500 1.1 0.14499999999985888\n",
      "20000 0.7 0.09999999999985551\n",
      "20500 0.5 0.09999999999985551\n",
      "21000 0.7 0.09999999999985551\n",
      "21500 0.1 0.09999999999985551\n",
      "22000 0.2 0.09999999999985551\n",
      "22500 1.3 0.09999999999985551\n",
      "23000 0.9 0.09999999999985551\n",
      "23500 0.6 0.09999999999985551\n",
      "24000 0.7 0.09999999999985551\n",
      "24500 1.1 0.09999999999985551\n",
      "25000 1.1 0.09999999999985551\n",
      "25500 0.6 0.09999999999985551\n",
      "26000 0.4 0.09999999999985551\n",
      "26500 1.1 0.09999999999985551\n",
      "27000 0.1 0.09999999999985551\n",
      "27500 0.4 0.09999999999985551\n",
      "28000 1.1 0.09999999999985551\n",
      "28500 0.5 0.09999999999985551\n",
      "29000 0.3 0.09999999999985551\n",
      "29500 0.9 0.09999999999985551\n",
      "30000 0.2 0.09999999999985551\n",
      "30500 1.0 0.09999999999985551\n",
      "31000 0.1 0.09999999999985551\n",
      "31500 0.9 0.09999999999985551\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-141c1f0c1df4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0;31m# 주요 신경망과 동일하게 타겟 신경망을 설정한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;31m# 물론 주요 신경망의 값은 tau 만큼만이 반영된다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                     \u001b[0mupdateTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetOps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mrAll\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-49634da8c238>\u001b[0m in \u001b[0;36mupdateTarget\u001b[0;34m(op_holder, sess)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupdateTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_holder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop_holder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/hyeongseokoh/.pyenv/versions/3.5.3/envs/algorithm-3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hyeongseokoh/.pyenv/versions/3.5.3/envs/algorithm-3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hyeongseokoh/.pyenv/versions/3.5.3/envs/algorithm-3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/hyeongseokoh/.pyenv/versions/3.5.3/envs/algorithm-3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hyeongseokoh/.pyenv/versions/3.5.3/envs/algorithm-3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "mainQN = Qnetwork()\n",
    "targetQN = Qnetwork()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "# 타겟 신경망을 업데이트하기 위한 값을 만든다\n",
    "targetOps = updateTargetGraph(trainables, tau)\n",
    "\n",
    "# 버퍼: episodeBuffer랑은 다른거다.\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "e = startE  # epsilon-greedy\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "indices_dict = {'state': 0,\n",
    "                'action': 1,\n",
    "                'reward': 2,\n",
    "                'next_state': 3,\n",
    "                'done':4}\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "    # 주요 신경망과 동일하게 타겟 신경망을 설정한다\n",
    "    updateTarget(targetOps, sess) \n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        # initialize episodes\n",
    "        s = env.reset()\n",
    "        s = flattenState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        episodeBuffer = experience_buffer()\n",
    "        \n",
    "        # Q-Network\n",
    "        while j < max_epLength:\n",
    "            j += 1\n",
    "            # epsilon-greedy\n",
    "            if np.random.rand(1) < e or \\\n",
    "               total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                # 신경망을 통해 Q 값을 가져오는 부분\n",
    "                a = sess.run(mainQN.predict,\n",
    "                             feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            \n",
    "            # run\n",
    "            s1, r, d = env.step(a)\n",
    "            s1 = flattenState(s1)\n",
    "            total_steps += 1\n",
    "            \n",
    "            # 버퍼에 현재 상태, 행동, 보상, 다음 상태, 종료 여부를 저장한다\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),\n",
    "                                         [1,5])) \n",
    "            \n",
    "            # 무작위 행동의 수를 넘으면 시작\n",
    "            if total_steps > pre_train_steps:\n",
    "                # 무작위 확률 값을 줄인다\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                # 총 걸음이 업데이트 수로 나누어 떨어지면 시작\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    # 경험으로부터 랜덤한 배치를 뽑는다\n",
    "                    trainBatch = myBuffer.sample(batch_size) \n",
    "\n",
    "                    ####### Double DQN ########\n",
    "                    # 주요 신경망에서 next_state에 대한 행동을 고른다.\n",
    "                    pred = sess.run(mainQN.predict,\n",
    "                                  feed_dict={mainQN.scalarInput: \n",
    "                                                 np.vstack(trainBatch[:, indices_dict['next_state']])})\n",
    "                    # 타겟 신경망에서 Q 값들을 얻는다.\n",
    "                    Q2 = sess.run(targetQN.Qout,\n",
    "                                  feed_dict={targetQN.scalarInput: \n",
    "                                                 np.vstack(trainBatch[:, indices_dict['next_state']])})\n",
    "\n",
    "                    end_multiplier = -(trainBatch[:,indices_dict['done']] - 1)  # 종료하면 굳이 해줄 필요가 없지..\n",
    "\n",
    "                    doubleQ = Q2[range(batch_size), pred]  # Q(s', argmax Q(s', a, theta1) , theta2)\n",
    "                    ############################\n",
    "                    targetQ = trainBatch[:, indices_dict['reward']] + (y * doubleQ * end_multiplier)\n",
    "                    # 우리의 타겟 값들과 함께 신경망을 업데이트해준다.\n",
    "                    # 행동들에 대해서 targetQ 값과의 차이를 통해 손실을 구하고 업데이트\n",
    "                    _ = sess.run(mainQN.updateModel,\n",
    "                                 feed_dict={mainQN.scalarInput: np.vstack(trainBatch[:, indices_dict['state']]),\n",
    "                                            mainQN.targetQ:     targetQ,\n",
    "                                            mainQN.actions:     trainBatch[:, indices_dict['action']]})\n",
    "                    \n",
    "                    # 주요 신경망과 동일하게 타겟 신경망을 설정한다.\n",
    "                    # 물론 주요 신경망의 값은 tau 만큼만이 반영된다.\n",
    "                    updateTarget(targetOps, sess)\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                break\n",
    "        \n",
    "        # 이 에피소드로부터의 모든 경험을 저장한다\n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print (\"Saved Model\")\n",
    "        # 최근 10 에피소드의 평균 보상\n",
    "        if len(rList) % 10 == 0:\n",
    "            print (total_steps, np.mean(rList[-10:]), e)\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "print (\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
